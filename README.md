# [Machine Learning DevOps Engineer Nanodegree](https://www.udacity.com/course/machine-learning-dev-ops-engineer-nanodegree--nd0821)

The Machine Learning DevOps Engineer Nanodegree program focuses on the software engineering fundamentals needed to successfully streamline the deployment of data and machine-learning models in a production-level environment. Students will build the DevOps skills required to automate the various aspects and stages of machine learning model building and monitoring over time.

## Projects  
### [Predict Customer Churn](https://github.com/FacenZHOU/predict-customer-churn)  
This project identifies credit card customers that are most likely to churn.
It includes a Python package for a ML project that follows coding (PEP8) and engineering best practices for implementing software (modular, documented, and tested).  

### [Build an ML Pipeline for Short-term Rental Prices in NYC](https://github.com/FacenZHOU/build-ml-pipeline-for-short-term-rental-prices)  
You are working for a property management company renting rooms and properties for short periods of time on various rental platforms. You need to estimate the typical price for a given property based on the price of similar properties. Your company receives new data in bulk every week. The model needs to be retrained with the same cadence, necessitating an end-to-end pipeline that can be reused.  

### [Deploying a Machine Learning Model on Heroku with FastAPI](https://github.com/FacenZHOU/deploying-a-ml-model-to-cloud-application-platform-with-fastapi)  
This project contains the development of a classification model on Census Bureau data. The main goal is to robustly deploy a machine learning model into production.  

This includes:
* testing the code using pytest
* deploying the model using the FastAPI package and creating API tests on Heroku
* incorporating the ML pipeline into a CI/CD framework using GitHub Actions.

### [ML Model Scoring and Monitoring](https://github.com/FacenZHOU/a-dynamic-risk-assessment-system)  
This project is about scoring processes to be performed after model deployment, such as model drift, and whether models need to be retrained and re-deployed. It deals with issues regarding data integrity, stability problems, timing problems, and dependency issues.
 
